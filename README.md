# ğŸ“Š Post Analyzer and Generator ğŸ–‹ï¸

This powerful tool analyzes your existing posts and helps generate new content that matches your unique writing style using advanced AI technology.

## âœ¨ Features

- ğŸ“ Analyze the writing style of your existing posts
- ğŸ¤– Generate new content with similar tone and style
- ğŸŒ Support for multiple languages
- ğŸ“ Flexible content length options
- ğŸš€ Powered by Llama3.2 open-source LLM

## ğŸ› ï¸ Set-up

To get started with the tool, follow these steps:

### 1. Obtain an API Key ğŸ”‘
- Visit [Groq API Console](https://console.groq.com/keys) to create and get your API_KEY.

### 2. Update .env File âš™ï¸
- Inside the `.env` file, update the value of `GROQ_API_KEY` with the API_KEY you obtained in the previous step.

### 3. Install Dependencies ğŸ“¦
- Install the required Python dependencies by running the following command:
  ```bash
  pip install -r requirements.txt
  ```

### 4. Run the Streamlit App ğŸš€
- After installing the dependencies, run the Streamlit app with the command:
  ```bash
  streamlit run main.py
  ```

This will start the app and you can begin analyzing and generating posts right away!

## ğŸ® Usage

Once the app is running, you can input your old posts and select the following options for the new post generation:

- **Text Length** ğŸ“:
  - âš¡ Short
  - ğŸ“„ Medium
  - ğŸ“š Long

- **Language** ğŸŒ:
  - ğŸ‡ºğŸ‡¸ English
  - ğŸ‡µğŸ‡± Polish
  - ğŸ‡³ğŸ‡± Dutch

The tool will analyze the writing style of your old posts and generate a new post in the chosen language and length that feels authentically "you".

## ğŸ’» Technologies Used

- **Llama3.2**: State-of-the-art open-source LLM for natural language processing
- **LangChain**: Framework for developing applications powered by language models
- **Streamlit**: Interactive web application framework for data apps
- **Groq Cloud**: Ultra-fast inference API for LLM deployment

## ğŸ“‹ Requirements

- Python 3.x
- Dependencies listed in `requirements.txt`

## ğŸ“ Project Structure

The project follows the directory structure outlined below:

```bash
project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_data/            # Store your original post data here
â”‚   â””â”€â”€ processed_data/      # Preprocessed data for analysis
â”‚
â”œâ”€â”€ .gitignore               # Files to be ignored by git
â”œâ”€â”€ README.md                # This documentation file
â”œâ”€â”€ few_shot.py              # Few-shot learning implementation
â”œâ”€â”€ llm_helper.py            # Helper functions for LLM interaction
â”œâ”€â”€ main.py                  # Main Streamlit application
â”œâ”€â”€ post_generator.py        # Post generation logic
â”œâ”€â”€ preprocess.py            # Data preprocessing utilities
â””â”€â”€ requirements.txt         # List of all dependencies needed to run the project
```

## ğŸ¤ Contribution

Feel free to contribute to this project by submitting issues or pull requests!
